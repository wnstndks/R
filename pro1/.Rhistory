result<-predict(ann,input)
result
ifelse(result>0.5,1,0)
output<-matrix(c(0,1,1,0)) #xor
output
ann<-nnet(input,output,maxit=1,size=2,decay=0.001)
ann
result<-predict(ann,input)
result
ifelse(result>0.5,1,0)
#인공신경망
df<-data.frame(
x1 = c(1:6),
x2=c(6:1),
y=factor(c('n','n','n','y','y','y'))
)
df
str(df)
library(nnet)
# x1,x2 : 독립변수, y: 종속변수
model <- nnet(y~.,df,size=1) # nnet(formula,dataset,size=hidden수)
# x1,x2 : 독립변수, y: 종속변수
model <- nnet(y~.,df,size=1) # nnet(formula,dataset,size=hidden수)
# x1,x2 : 독립변수, y: 종속변수
model <- nnet(y~.,df,size=1) # nnet(formula,dataset,size=hidden수)
model
# x1,x2 : 독립변수, y: 종속변수
model <- nnet(y~.,df,size=10) # nnet(formula,dataset,size=hidden수)
model
summary(model)
# x1,x2 : 독립변수, y: 종속변수
model <- nnet(y~.,df,size=100) # nnet(formula,dataset,size=hidden수), 노드의 개수를 늘려주면 그 만큼 연산량이 늘어난다,
#정확도도 높아짐 그러나 너무 많으면 안됨
model
summary(model)
install.packages("devtools")
library(devtools)
plot.nnet(summary(model))
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(summary(model))
# x1,x2 : 독립변수, y: 종속변수
model <- nnet(y~.,df,size=10) # nnet(formula,dataset,size=hidden수), 노드의 개수를 늘려주면 그 만큼 연산량이 늘어난다,
#정확도도 높아짐 그러나 너무 많으면 안됨
model
summary(model)
install.packages("devtools")
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(summary(model))
# x1,x2 : 독립변수, y: 종속변수
model <- nnet(y~.,df,size=2) # nnet(formula,dataset,size=hidden수), 노드의 개수를 늘려주면 그 만큼 연산량이 늘어난다,
#정확도도 높아짐 그러나 너무 많으면 안됨
model
summary(model)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
plot.nnet(summary(model))
#분류 모형 예측
model$fitted.values #분류모델 적합값
predict(model,df)
ifelse(predict(model,df)->0.5,1,0)
ifelse(predict(model,df)>0.5,1,0)
ifelse(predict(model,df)>0.5,"n","y")
ifelse(predict(model,df)>0.5,"y","n")
pre<-predict(model,df,type="class")
pre
table(pre,df$y)
# iris dataset으로 출력겨과가 세개의 범주로 처리
data(iris)
iris
head(iris,n=2)
unique(iris$Species)
dim(iris)
#분류 모델 작성 전, dataset을 train /test로 분리(7:3)
set.seed(123)
idx<-sample(1:nrow)
idx<-sample(1:nrow(iris))
idx<-sample(1:nrow(iris),nrow(iris)*0.7)
idx
train<-iris[idx,]
test<-iris[-idx]
test<-iris[-idx,]
test
nrow(train)
nrow(test)
library(nnet)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
model_1<-nnet(Species~.,train,size=1)
model_1
plot.nnet(summary(model_1))
#node를 3개 사용한 경우
model_2<-nnet(Species~.,train,size=3)
model_2 #softmax modelling : 출력값은 확률
summary(model_2)
plot.nnet(summary(model_2))
# 분류 모델 평가
pred1<-predict(model_1)
# 분류 모델 평가
pred1<-predict(model_1,test,type = "class")
t1<-table(pred1,test$Species)
t1
sum(diag(t1))/nrow(test)
pred2<-predict(model_2,test,type = "class")
t2<-table(pred2,test$Species)
t2
sum(diag(t2))/nrow(test) #0.9777778
#다른 방법
model_3<-nnet(train[,1:4])
#다른 방법
model_3<-nnet(train[,1:4],class.ind(train$Species))
#다른 방법
model_3<-nnet(train[,1:4],class.ind(train$Species),size=3,softmax = TRUE)
model_3
pr<-predict(model_3,newdata=test[,1:4],type="class")
pr
table(pr,test$Species)
install.packages("neuralnet")
head(iris,2)
library(neuralnet)
head(iris,2)
#Species를 숫자형태로 변환
iris$Species[iris$Species=='Setosa']
unique(iris$Species)
#Species를 숫자형태로 변환
iris$Species[iris$Species=='setosa']<-1
#Species를 숫자형태로 변환
iris$Species2[iris$Species=='setosa']<-1
iris$Species2[iris$Species=='versicolor']<-2
iris$Species2[iris$Species=='virginica']<-3
head(iris,2)
head(iris,2)
head(iris,2)
unique(iris$Species)
#Species를 숫자형태로 변환
iris$Species2[iris$Species=='setosa']<-1
#Species를 숫자형태로 변환
iris$Species2[iris$Species=='setosa']<-1
iris$Species2[iris$Species=='versicolor']<-2
iris$Species2[iris$Species=='virginica']<-3
head(iris,2)
iris$Species<-NULL
head(iris,2)
unique(iris$Species2)
#Species를 숫자형태로 변환
iris$Species2[iris$Species=='setosa']<-1
unique(iris$Species2)
print((x-min(x))/(max(x)-min(x)))
#독립변수에 대한 정규화를 실시 (대개의 경우 정규화 또는 표준화를 하면 모델의 성능이 향상)
normal_func<-function(x){
print((x-min(x))/(max(x)-min(x)))
}
normal_func(1,2,3,50,100)
normal_func(c(1,2,3,50,100))
normal_func(c(1,2,3))
#독립변수에 대한 정규화를 실시 (대개의 경우 정규화 또는 표준화를 하면 모델의 성능이 향상)
normal_func<-function(x){
#print((x-min(x))/(max(x)-min(x)))
return (x-min(x))/(max(x)-min(x))
}
normal_func(c(1,2,3))
train_nor<-as.data.frame(lapply(train,normal_func))
set.seed(123)
idx<-sample(1:nrow(iris),nrow(iris)*0.7)
idx
train<-iris[idx,]
test<-iris[-idx,]
nrow(train) #105
nrow(test) #45
train_nor<-as.data.frame(lapply(train,normal_func))
train_nor
head(train_nor,5)
#검정 데이터를 정규화
test_nor<-as.data.frame(lapply(test,normal_func))
head(test_nor,5)
help("neuralnet")
model<-neuralnet(Species2~.,data=train_nor)
model<-neuralnet(Species2~.,data=train_nor,hidden = 1,algorithm = 'backprop',
learningrate = 0.01)
model
model<-neuralnet(Species2~.,data=train_nor,hidden = 1,algorithm = 'backprop',
learningrate = 0.01)
model
plot(model)
# neuralnet: iris dataset을 사용
library(neuralnet)
#분류 모델 작성 전, dataset을 train /test로 분리(7:3)
set.seed(123)
idx<-sample(1:nrow(iris),nrow(iris)*0.7)
idx
train<-iris[idx,]
test<-iris[-idx,]
nrow(train) #105
nrow(test) #45
head(train,3)
#T,F가 있는 열 3개 추가
train<-cbind(train,train$Species=='setosa')
#T,F가 있는 열 3개 추가
train<-cbind(train,train$Species=='setosa')
train<-cbind(train,train$Species=='versicolor')
train<-cbind(train,train$Species=='virginica')
train
names(train[6:8]<-c('setosa','versicolor','virginica'))
train
head(train,3)
names(train)[6:8]<-c('setosa','versicolor','virginica'))
names(train)[6:8]<-c('setosa','versicolor','virginica')
head(train,3)
#분류 모델 작성 전, dataset을 train /test로 분리(7:3)
set.seed(123)
idx<-sample(1:nrow(iris),nrow(iris)*0.7)
idx
train<-iris[idx,]
test<-iris[-idx,]
nrow(train) #105
nrow(test) #45
head(train,3)
names(train)[6:8]<-c('setosa','versicolor','virginica')
head(train,3)
#T,F가 있는 열 3개 추가
train<-cbind(train,train$Species=='setosa')
train<-cbind(train,train$Species=='versicolor')
train<-cbind(train,train$Species=='virginica')
names(train)[6:8]<-c('setosa','versicolor','virginica')
head(train,3)
#model 생성
model<-neuralnet(setosa+versicolor+virginica~Sepal.Length +Sepal.Width +Petal.Length+ Petal.Width,
data=train,hidden=3)
plot(model)
#예측: neuralnet은 predict이 아니라 compute를 지원
head(test,2)
comp<-compute(model,test[-5])
pred_weight<-comp$net.result
idx<-apply(pred_weight, 1,which.max)
idx
pred<-c('setosa','versicolor','verginica')
pred<-c('setosa','versicolor','verginica')[idx]
pred
c('setosa','versicolor','verginica')[1]
c('setosa','versicolor','verginica')[3]
t<-table(pred,test$Species)
t
sum(diag(t))/nrow(test)
c('setosa','versicolor','virginica')[3]
pred<-c('setosa','versicolor','virginica')[idx]
pred
t<-table(pred,test$Species)
t
sum(diag(t))/nrow(test)
#미지의 새로운 값으로 분류 수행
my<-test
my<-my[c(1:3),]
my<-edit(my)
my
mycomp<-compute(model,my[-5])
mypred<-mycomp$net.result
idx2<-apply(mypred,1,which.max)
ix2
idx2
pred2<-c('setosa','versicolor','virginica')[idx2]
pred2
my
my<-edit(my)
my
mycomp<-compute(model,my[-5])
mypred<-mycomp$net.result
idx2<-apply(mypred,1,which.max)
idx2
pred2<-c('setosa','versicolor','virginica')[idx2]
pred2
# 유클리디안 거리 계산법
x<-matrix(1:16,nrow=4)
x
?dist
dist
plot(dist)
dist
dist<-dist(x, method=x) # euclidean maximum manhattan canberra binary, minkowski
x
dist<-dist(x, method=x) # euclidean maximum manhattan canberra binary, minkowski
dist<-dist(x, method="euclidean") # euclidean maximum manhattan canberra binary, minkowski
dist
plot(dist)
text(dist,c(LETTERS[1:6]),pos=2)
sqrt(sum((x[1,])))
sqrt(sum((x[1,]-x[4,])^2))
getwd()
setwd()
setwd("C:/work/rsou/pro1")
txt1<-read.csv("testdata/cluster_ex.csv")
txt1
txt1<-read.csv("testdata/cluster_ex.csv",encoding="utf-8")
txt1
txt1<-read.csv("testdata/cluster_ex.csv",encoding="utf-8")
txt1
txt1<-read.csv("testdata/cluster_ex.csv",encoding="utf-8")
txt1
plot(txt1[,c(2:3)])
plot(txt1[,c(2:3)],xlabel='국어',ylabel='영어')
plot(txt1[,c(2:3)],xlabel='국어',ylabel='영어',main='학생점수 산포도')
plot(txt1[,c(2:3)],xlabel='국어',ylabel='영어',main='학생점수 산포도',xlim=c(30,100),ylim=c(30,100))
plot(txt1[,c(2:3)],xlabel='국어',ylabel='영어',main='학생점수 산포도',xlim=c(50,100),ylim=c(30,100))
plot(txt1[,c(2:3)],xlabel='국어',ylabel='영어',main='학생점수 산포도',
xlim=c(50,100),ylim=c(30,100))
# 홍길동과 이기자 두 학생의 거리 계산
text(txt1[,2])
# 홍길동과 이기자 두 학생의 거리 계산
text(txt1[,2],txt1[,3])
# 홍길동과 이기자 두 학생의 거리 계산
text(txt1[,2],txt1[,3],labels=abbreviate(rownames(txt1)),cex=0.8,pos=1,col='blue')
text(txt1[,2],txt1[,3],labels=txt[,1], cex=0.8,pos=1,col='blue')
text(txt1[,2],txt1[,3],labels=txt1[,1], cex=0.8,pos=1,col='blue')
points(txt1[1,2],txt1[1,3],col='red',pch=19)
points(txt1[2,2],txt1[2,3],col='red',pch=19)
dist_mht<-dist(txt1[c(1:2),c(2:3)])
dist_mht
dist_mht<-dist(txt1[c(1:2),c(2:3)],method="manhattan")
dist_mht
dist_euc<-dist(txt1[c(1:2),c(2:3)],method="euclidean")
dist_euc
x<-c(1,2,2,4,5)
y<-c(1,1,4,3,4)
xy<-data.frame(cbind(x,y))
xy
plot(xy,pch=20)
plot(xy,pch=20,xlab=c("x값"),ylab=c("y값"))
plot(xy,pch=20,xlab=c("x값"),ylab=c("y값"),xlim=c(0,6),ylim=(0,6))
text(xy[,1],xy[,2],labels=abbreviate(rownames(xy)),cex=0.8,pos=1,col="blue")
plot(xy,pch=20,xlab=c("x값"),ylab=c("y값"),xlim=c(0,6),ylim=(0,6))
plot(xy,pch=20,xlab=c("x값"),ylab=c("y값"),xlim=c(0,6),ylim=c(0,6))
text(xy[,1],xy[,2],labels=abbreviate(rownames(xy)),cex=0.8,pos=1,col="blue")
abline(v=3,col="gray",lty=2)
dist(xy)
dist(xy)^2
#덴드로그램으로 군집 확인
hc_res<-hclust(dist)
#덴드로그램으로 군집 확인
hc_res<-hclust(dist(xy)^2)
#덴드로그램으로 군집 확인
?hclust
hc_res
plot(hc_res)
plot(hc_res,hang=-1)
#------------
body <- read.csv("testdata/bodycheck.csv")
#------------
body <- read.csv("testdata/bodycheck.csv",encoding = "utf-8")
head(body)
head(body,3)
#-----중학교 1학년 학생들의 신체검사 자료로 군집화-------
body <- read.csv("testdata/bodycheck.csv",encoding = "utf-8")
head(body,3)
str(body)
d<-dist(body[,-1],method="euclidian")
d
d<-dist(body[,-1],method="euclidiean")
d
hc<-hclust(d,method="complete")
hc
plot(hc)
plot(hc,hang=-1)
rect.hclust(hc,k=3,border="red")
library(cluster)
g1<-subset(body, 번호=10|번호=4|번호=8|번호=1번호=15)
g1<-subset(body, 번호=10|번호=4|번호=8|번호=1|번호=15)
g1<-subset(body, 번호==10|번호==4|번호==8|번호==1|번호==15)
g2<-subset(body, 번호==11|번호==3|번호==5|번호==6|번호==14)
g3<-subset(body, 번호==2|번호==9|번호==13|번호==7|번호==12)
g1
g1[2:5]
g2[2:5]
g3[2:5]
#---------iris dataset으로 군집화--------------
idist<-dist(iris[,1:4])
idist
hc<-hclust(idist)
plot(hc,hang=-1)
rect.hclust(hc,k=2,border="red")
rect.hclust(hc,k=3,border="red")
ghc
# 군집수 생성
ghc<-cutree(hc,k=3) # 3개로 분리
ghc
head(iris,3)
iris$ghc<-ghc
head(iris,3)
table(ghc)
head(g1,5)
g1<-subset(iris,ghc==1)
g2<-subset(iris,ghc==2)
g3<-subset(iris,ghc==3)
head(g2,5)
head(g3,5)
head(g1,5)
summary(g2)
summary(g3)
data
d_data<-dist(data,method="euclidian")
d_data<-dist(data,method="euclidean")
data<-read.csv("testdata/exam.csv",sep=" ",header=T)
data
d_data<-dist(data,method="euclidean")
d_data
gra_data<-cmdscale(d_data) #공간적 배치가 가능한 형태로 변환
gra_data
plot(gra_data)
text(gra_data)
plot(gra_data,type="n")
text(gra_data)
#학생들의 성적으로 k-means 군집분석
data$avg<-apply(data[,[2:5]])
#학생들의 성적으로 k-means 군집분석
data$avg<-apply(data[[,2:5],as.numeric()])
#학생들의 성적으로 k-means 군집분석
data$avg<-apply(data[[,2:5],as.numeric())
#학생들의 성적으로 k-means 군집분석
data$avg<-apply(data[[,2:5],as.numeric
#학생들의 성적으로 k-means 군집분석
data$avg<-sapply(data[,2:5],as.numeric)
data
data<-read.csv("testdata/exam.csv",sep=" ",header=T)
data
data<-read.csv("testdata/exam.csv",sep=" ",header=T)
data
d_data<-dist(data,method="euclidean")
d_data
gra_data<-cmdscale(d_data) #공간적 배치가 가능한 형태로 변환
gra_data
plot(gra_data,type="n")
text(gra_data)
#학생들의 성적으로 k-means 군집분석
data$avg<-sapply(data[,2:5],as.numeric)
data
#학생들의 성적으로 k-means 군집분석
str(data)
data$avg<-sapply(data[,2:5],as.numeric)
data
data$avg<-apply(data[,2:5],as.numeric)
data
data$avg<-apply(data[,2:5],1,mean)
data
install.packages("NbClust")
library(NbClust)
data_s<-scale(data[2:5]) #표준화
data_s
#best k값 얻기
nc<-NbClust(data_s)
nc
#best k값 얻기
nc<-NbClust(data_s,min.nc = 2,)
nc
#best k값 얻기
nc<-NbClust(data_s,min.nc = 2,max.nc=5,method='kmeans')
nc
plot(table(nc$Best.nc[1,]))
plot(table(nc$Best.nc[1,]))
nc
#best k값 얻기
nc<-NbClust(data_s,min.nc = 2,max.nc=5,method='kmeans')
nc
plot(table(nc$Best.nc[1,]))
#best k값 얻기
nc<-NbClust(data_s,min.nc = 2,max.nc=5,method='kmeans')
#모델 생성
model_kmeans<-kmeans(data[,c("bun","avg")],4)
model_kmeans
table(model_kmeans$cluster)
cluster<-model_kmeans$cluster
cluster
kmeans_df<-cbind(cluster)
kmeans_df<-cbind(cluster,data[,c("bun","avg")])
kmeans_df
str(kmeans_df)
kmeans_df <-transform(kmeans_df)
kmeans_df
kmeans_df <-transform(kmeans_df,cluster=as.factor(cluster))
kmeans_df
kmeans_df
str(kmeans_df)
library(ggplot2)
df_plot<-ggplot(data = kmeans_df)
df_plot<-ggplot(data = kmeans_df,aex(x=bun,y=avg,col=cluster))
df_plot<-ggplot(data = kmeans_df,axis(x=bun,y=avg,col=cluster))
df_plot<-ggplot(data = kmeans_df,aes(x=bun,y=avg,col=cluster))
df_plot
+geom_point(aes(shape=factor(cluster),size=4))
df_plot
+ geom_point(aes(shape=factor(cluster),size=4))
df_plot
+ geom_point(aes(shape=factor(cluster),size=4))+
ggtitle("학생점수분포")
df_plot
+ geom_point(aes(shape=factor(cluster),size=4))+
ggtitle("학생점수분포")
df_plot
kmeans_df <-transform(kmeans_df,cluster=as.factor(cluster))#cluster열을 factor타입으로 변환
kmeans_df
str(kmeans_df)
library(ggplot2)
df_plot<-ggplot(data = kmeans_df,aes(x=bun,y=avg,col=cluster))
+ geom_point(aes(shape=factor(cluster),size=4))+
ggtitle("학생점수분포")
df_plot
kmeans_df<-cbind(cluster,data[,c("bun","avg")])
kmeans_df
str(kmeans_df)
df_plot<-ggplot(data = kmeans_df,aes(x=bun,y=avg,col=cluster))
df_plot<-ggplot2(data = kmeans_df,aes(x=bun,y=avg,col=cluster))
+ geom_point(aes(shape=factor(cluster),size=4))+
ggtitle("학생점수분포")
df_plot<-ggplot(data = kmeans_df,aes(x=bun,y=avg,col=cluster))
df_plot
